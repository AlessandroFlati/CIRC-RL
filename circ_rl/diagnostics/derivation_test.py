"""Derivation test: verify policy trajectories match predictions.

Tests whether trajectories generated by the analytic policy follow
the trajectories predicted by the hypothesis.

See ``CIRC-RL_Framework.md`` Section 3.9.2.
"""

from __future__ import annotations

import contextlib
from dataclasses import dataclass
from typing import TYPE_CHECKING

import numpy as np
from loguru import logger

if TYPE_CHECKING:
    from circ_rl.analytic_policy.analytic_policy import AnalyticPolicy
    from circ_rl.environments.data_collector import ExploratoryDataset
    from circ_rl.hypothesis.expression import SymbolicExpression


@dataclass(frozen=True)
class DerivationTestResult:
    """Result of the derivation test.

    :param passed: Whether policy trajectories match hypothesis predictions.
    :param mean_divergence: Mean divergence between predicted and observed
        state sequences under the policy.
    :param max_divergence: Maximum divergence observed.
    """

    passed: bool
    mean_divergence: float
    max_divergence: float


class DerivationTest:
    r"""Test whether the analytic policy produces trajectories consistent
    with the dynamics hypothesis.

    If this fails but the premise test passes, the derivation method
    (LQR, MPC) has a bug or the constraints are interfering unexpectedly.

    See ``CIRC-RL_Framework.md`` Section 3.9.2.

    :param divergence_threshold: Maximum allowed mean divergence.
    :param horizon: Number of steps to simulate. Default 50.
    """

    def __init__(
        self,
        divergence_threshold: float = 1.0,
        horizon: int = 50,
    ) -> None:
        self._threshold = divergence_threshold
        self._horizon = horizon

    def test(
        self,
        policy: AnalyticPolicy,
        dynamics_expressions: dict[int, SymbolicExpression],
        dataset: ExploratoryDataset,
        state_feature_names: list[str],
        variable_names: list[str],
        test_env_ids: list[int],
    ) -> DerivationTestResult:
        """Run the derivation test.

        Simulates the policy forward using the dynamics hypothesis and
        compares predicted vs. hypothesis-predicted state sequences.

        :param policy: The analytic policy.
        :param dynamics_expressions: Validated dynamics expressions.
        :param dataset: Test environment data (for initial states).
        :param state_feature_names: State feature names.
        :param variable_names: Variable names for expression evaluation.
        :param test_env_ids: Environment IDs to test.
        :returns: DerivationTestResult.
        """
        from circ_rl.hypothesis.trajectory_prediction import (
            TrajectoryPredictionTest,
        )

        len(state_feature_names)
        actions_2d = (
            dataset.actions if dataset.actions.ndim == 2
            else dataset.actions[:, np.newaxis]
        )
        action_dim = actions_2d.shape[1]

        # Compile dynamics callables
        callables: dict[int, object] = {}
        for dim_idx, expr in dynamics_expressions.items():
            with contextlib.suppress(ValueError):
                callables[dim_idx] = expr.to_callable(variable_names)

        divergences: list[float] = []

        for env_id in test_env_ids:
            mask = dataset.env_ids == env_id
            if not np.any(mask):
                continue

            env_states = dataset.states[mask]
            n_e = env_states.shape[0]

            if n_e < self._horizon:
                continue

            # Start from multiple initial states
            n_starts = min(5, n_e // self._horizon)
            rng = np.random.RandomState(env_id)
            starts = rng.choice(n_e - self._horizon, size=max(1, n_starts))

            for start in starts:
                state = env_states[start].copy()
                total_div = 0.0

                for _t in range(self._horizon):
                    # Get action from the policy
                    action = policy.get_action(state, env_id)

                    # Predict next state using dynamics hypothesis
                    x_row = TrajectoryPredictionTest._build_input_row(
                        state, action, variable_names,
                        state_feature_names, action_dim, None,
                    )

                    predicted_next = state.copy()
                    for dim_idx, func in callables.items():
                        try:
                            delta = float(func(x_row)[0])  # type: ignore[operator]
                            predicted_next[dim_idx] = state[dim_idx] + delta
                        except Exception:  # noqa: S110
                            pass

                    # The "observed" state under the policy is what we'd
                    # get from the dynamics hypothesis. If the derivation
                    # is correct, the policy should produce actions that
                    # keep the system on the predicted trajectory.
                    state = predicted_next

                    # Compare with applying policy to the predicted state
                    # (self-consistency check)
                    action2 = policy.get_action(predicted_next, env_id)
                    div = float(np.sqrt(np.sum((action - action2) ** 2)))
                    total_div += div

                avg_div = total_div / self._horizon
                divergences.append(avg_div)

        if not divergences:
            return DerivationTestResult(
                passed=True, mean_divergence=0.0, max_divergence=0.0,
            )

        mean_div = float(np.mean(divergences))
        max_div = float(np.max(divergences))
        passed = mean_div <= self._threshold

        logger.info(
            "Derivation test: mean_div={:.4f}, max_div={:.4f}, "
            "threshold={:.4f}, passed={}",
            mean_div, max_div, self._threshold, passed,
        )

        return DerivationTestResult(
            passed=passed,
            mean_divergence=mean_div,
            max_divergence=max_div,
        )
